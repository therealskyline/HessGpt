"""
üöÄ HessGPT Web Interface - Flask Server
‚úÖ Support multi-mod√®les (124M, 50M, 20M)
‚úÖ API REST + Interface HTML moderne
‚úÖ Pr√™t pour production
"""

from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
import torch
import torch.nn.functional as F
from transformers import GPT2Tokenizer
import sys
import os

# Importer ton mod√®le
sys.path.append('./Core/Model')
from HessGpt import HessGPT

app = Flask(__name__)
CORS(app)

# ============================================
# CONFIGURATION DES MOD√àLES
# ============================================

MODELS_CONFIG = {
    '124M': {
        'path': './Models/124M/Hessgpt_Final_SFT.pt',
        'config': {
            'vocab_size': 50257,
            'embed_dim': 768,
            'num_heads': 12,
            'num_layers': 12,
            'max_seq_len': 1024,
            'dropout': 0.05,
        },
        'description': 'Mod√®le large - V3'
    },
    '50M': {
        'path': './Models/50M/Hessgpt_Final_SFT.pt',
        'config': {
            'vocab_size': 50257,
            'embed_dim': 512,
            'num_heads': 8,
            'num_layers': 8,
            'max_seq_len': 1024,
            'dropout': 0.05,
        },
        'description': 'Mod√®le moyen - V2'
    },
    '20M': {
        'path': './Models/20M/Hessgpt_Final_SFT.pt',
        'config': {
            'vocab_size': 50257,
            'embed_dim': 384,
            'num_heads': 6,
            'num_layers': 6,
            'max_seq_len': 1024,
            'dropout': 0.05,
        },
        'description': 'Mod√®le l√©ger - V1'
    }
}

# Device
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# Stockage des mod√®les charg√©s
loaded_models = {}
current_model_name = None

# ============================================
# INITIALISATION
# ============================================

print("="*60)
print("üöÄ D√âMARRAGE SERVEUR HessGPT MULTI-MOD√àLES")
print("="*60)
print(f"‚úÖ Device: {DEVICE}")

# Tokenizer (commun √† tous les mod√®les)
tokenizer = GPT2Tokenizer.from_pretrained("./Core/Tokenizer")
tokenizer.pad_token = tokenizer.eos_token

# V√©rifier quels mod√®les sont disponibles
available_models = []
for model_name, model_info in MODELS_CONFIG.items():
    if os.path.exists(model_info['path']):
        available_models.append(model_name)
        print(f"‚úì Mod√®le {model_name} trouv√©: {model_info['path']}")
    else:
        print(f"‚úó Mod√®le {model_name} absent: {model_info['path']}")

if not available_models:
    print("‚ùå ERREUR: Aucun mod√®le trouv√©!")
    print("üìÅ V√©rifiez la structure: ./Models/[124M|50M|20M]/Hessgpt_Final_SFT.pt")
    sys.exit(1)

print(f"\n‚úÖ {len(available_models)} mod√®le(s) disponible(s): {', '.join(available_models)}")
print("="*60)

# ============================================
# GESTION DES MOD√àLES
# ============================================

def load_model(model_name):
    """Charge un mod√®le sp√©cifique"""
    global current_model_name
    
    if model_name not in MODELS_CONFIG:
        raise ValueError(f"Mod√®le {model_name} inconnu")
    
    if model_name not in available_models:
        raise ValueError(f"Mod√®le {model_name} non disponible")
    
    # Si d√©j√† charg√©, le retourner
    if model_name in loaded_models:
        current_model_name = model_name
        return loaded_models[model_name]
    
    print(f"\n‚è≥ Chargement du mod√®le {model_name}...")
    
    model_info = MODELS_CONFIG[model_name]
    
    # Charger le checkpoint
    checkpoint = torch.load(model_info['path'], map_location=DEVICE)
    
    # Cr√©er et initialiser le mod√®le
    model = HessGPT(**model_info['config']).to(DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # Stocker
    loaded_models[model_name] = {
        'model': model,
        'config': model_info['config'],
        'checkpoint': checkpoint
    }
    current_model_name = model_name
    
    print(f"‚úÖ Mod√®le {model_name} charg√©!")
    print(f"   Epoch: {checkpoint.get('epoch', 'N/A')}")
    print(f"   Val Loss: {checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'N/A'))}")
    
    return loaded_models[model_name]

# Charger le premier mod√®le disponible par d√©faut
default_model = available_models[0]
load_model(default_model)

# ============================================
# FONCTION DE G√âN√âRATION
# ============================================

def generate_response(prompt, model_name=None, max_tokens=100, temperature=0.7, top_k=50, top_p=0.9):
    """
    G√©n√®re une r√©ponse avec le mod√®le HessGPT
    """
    if model_name is None:
        model_name = current_model_name
    
    # Charger le mod√®le si n√©cessaire
    model_data = load_model(model_name)
    model = model_data['model']
    config = model_data['config']
    
    model.eval()
    
    # Formater le prompt (style Alpaca)
    formatted_prompt = f"Instruction: {prompt}\nResponse:"
    
    # Tokenization
    tokens = tokenizer.encode(formatted_prompt, return_tensors='pt').to(DEVICE)
    generated = tokens[0].tolist()
    
    with torch.no_grad():
        for _ in range(max_tokens):
            input_ids = torch.tensor([generated], dtype=torch.long).to(DEVICE)
            
            if input_ids.size(1) > config['max_seq_len']:
                input_ids = input_ids[:, -config['max_seq_len']:]
            
            logits, _ = model(input_ids)
            next_token_logits = logits[0, -1, :]
            
            # Temp√©rature
            next_token_logits = next_token_logits / temperature
            
            # Anti-r√©p√©tition
            for token in set(generated[-50:]):
                next_token_logits[token] /= 1.2
            
            # Top-k filtering
            if top_k > 0:
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Top-p filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Sampling
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()
            
            if next_token == tokenizer.eos_token_id:
                break
            
            generated.append(next_token)
    
    # D√©coder
    full_text = tokenizer.decode(generated, skip_special_tokens=True)
    
    if "Response:" in full_text:
        response = full_text.split("Response:")[-1].strip()
    else:
        response = full_text[len(formatted_prompt):].strip()
    
    return response

# ============================================
# ROUTES FLASK
# ============================================

@app.route('/')
def home():
    """Page d'accueil"""
    return render_template('index.html')

@app.route('/models', methods=['GET'])
def get_models():
    """Retourne la liste des mod√®les disponibles"""
    models_list = []
    for model_name in available_models:
        model_info = MODELS_CONFIG[model_name]
        models_list.append({
            'name': model_name,
            'description': model_info['description'],
            'active': model_name == current_model_name
        })
    
    return jsonify({
        'models': models_list,
        'current': current_model_name
    })

@app.route('/switch_model', methods=['POST'])
def switch_model():
    """Change de mod√®le actif"""
    try:
        data = request.get_json()
        model_name = data.get('model')
        
        if not model_name:
            return jsonify({'error': 'Nom de mod√®le manquant', 'success': False}), 400
        
        load_model(model_name)
        
        return jsonify({
            'success': True,
            'model': model_name,
            'message': f'Mod√®le {model_name} activ√©'
        })
    
    except Exception as e:
        return jsonify({'error': str(e), 'success': False}), 500

@app.route('/generate', methods=['POST'])
def generate():
    """API de g√©n√©ration"""
    try:
        data = request.get_json()
        
        if not data or 'prompt' not in data:
            return jsonify({'error': 'Prompt manquant', 'success': False}), 400
        
        prompt = data['prompt'].strip()
        if not prompt:
            return jsonify({'error': 'Prompt vide', 'success': False}), 400
        
        # Param√®tres
        model_name = data.get('model', current_model_name)
        max_tokens = min(int(data.get('max_tokens', 100)), 500)
        temperature = max(0.1, min(float(data.get('temperature', 0.7)), 1.0))
        
        # G√©n√©ration
        response = generate_response(
            prompt=prompt,
            model_name=model_name,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return jsonify({
            'response': response,
            'success': True,
            'model': current_model_name,
            'params': {
                'max_tokens': max_tokens,
                'temperature': temperature
            }
        })
    
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration: {e}")
        return jsonify({'error': f'Erreur serveur: {str(e)}', 'success': False}), 500

@app.route('/clear', methods=['POST'])
def clear():
    """Efface l'historique"""
    return jsonify({'success': True, 'message': 'Conversation effac√©e'})

@app.route('/health', methods=['GET'])
def health():
    """Health check"""
    return jsonify({
        'status': 'healthy',
        'device': DEVICE,
        'current_model': current_model_name,
        'available_models': available_models
    })

@app.route('/info', methods=['GET'])
def info():
    """Informations sur le mod√®le actuel"""
    if current_model_name not in loaded_models:
        return jsonify({'error': 'Aucun mod√®le charg√©'}), 500
    
    model_data = loaded_models[current_model_name]
    checkpoint = model_data['checkpoint']
    
    return jsonify({
        'model': current_model_name,
        'description': MODELS_CONFIG[current_model_name]['description'],
        'epoch': checkpoint.get('epoch', 'N/A'),
        'val_loss': checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'N/A')),
        'config': model_data['config'],
        'device': DEVICE,
        'samples_seen': checkpoint.get('total_samples_seen', 'N/A')
    })

# ============================================
# D√âMARRAGE SERVEUR
# ============================================

if __name__ == '__main__':
    print("\n" + "="*60)
    print("üåê Serveur d√©marr√©!")
    print("="*60)
    print("üìç Interface: http://localhost:5000")
    print("üìç API: http://localhost:5000/generate")
    print("üìç Mod√®les: http://localhost:5000/models")
    print("üìç Health: http://localhost:5000/health")
    print(f"üìç Mod√®le actif: {current_model_name}")
    print("="*60)
    print("\n‚ö†Ô∏è  Utiliser CTRL+C pour arr√™ter\n")
    
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=False,
        threaded=True
    )